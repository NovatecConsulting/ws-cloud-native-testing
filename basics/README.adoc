= Testautomation Basics

== Why?

Why should we test our code and why should those tests be automated?

> Software Testing is not about finding bugs.
It's about delivering great software.
-- Harry Robinson

Software which does not work correctly is at best bad publicity and at worst kills people.
Any kind of testing, automated or otherwise, is done to mitigate the risk of having written _wrong_ code before letting that code loose on the world.

> Instead of looking for bugs, why not focus on preventing them?
-- Jeff Morgan

Testing software after it was programmed and is already potentially shippable does not allow for the fast feedback cycles needed to continuously deliver that software to customers.
Developers are usually busy with the next feature by the time a bug report comes in.
Which just further delays the delivery of that version of the software.

A much better approach is to test as part of programming.
That's where automated tests come into play.
Developers are very good at two things: _writing code_ and _automating boring tasks_
Repeatedly testing the same things manually is very boring.
That's why developers should be intrinsically motivated to automate their testing activities.

Writing automated tests during development has several key benefits:

- No need to manually test what has already been automatically tested every time the software is built.
- Executing huge amounts of tests in minutes, which would take a human tester days.
- Having to write testable production code usually means that the code is also better structured.
- If you have a good suite of _efficient_ automated tests, you'll never be afraid of refactoring your code to keep it maintainable for future generations.

Manual testing still has its place in the software development lifecycle.
But that place is not with testing for regression (testing everything everytime something changes to see if anything was broken).
They are useful for:

* _Explorative Testing_ on production or at least close to production systems to find bugs and potential for improvement.
* _Approving_ of changes (e.g. user stories) to the software by qualified people (e.g. product owner, business analyst etc.) if required.

Automated tests must build the backbone of your software development process though.
Every found bug and resulting bugfix will add new automated tests to protect against regression.
Similarly, whenever one or more missing automated tests are discovered in manual testing efforts, they need to be added as well.
Every repeated test must be automated!

This will allow you to move faster by deploying more often, changing whatever needs to change whenever it needs to and with much more confidence.

== How?

=== Kinds of Automated Tests

_Unit-_, _Component-_, _Integration-_, _System-_, _End-to-End-_, _Acceptance-_, _Behaviour-_, _Property-_, _Contract-_, _Snapshot-_, _Performance-_, _Security-_ and _Architecture-Tests_ as well as things like _Static Code Analysis_.

These are a lot of labels for seemingly complex things, that actually come down to just a handful of properties:

* *scope* - _"Where does the test start and where does it end?"_
** pure function
** class method
** class
** package
** module
** deployment unit
** repository
** system

* *aspect under test*
** functionality - _"Does my code do what I want it to?"_
** integration - _"Does my code integrate with other code, framework features, technologies etc. as I intended it to?"_
** rules - _"Does my code follow our defined rules? (e.g. architecture, security, style)"_
** performance - _"Does my code execute as fast as I need it to?"_

The goal of efficient and sustainable automated testing is to keep the following **costs** down, while still covering all relevant risks:

* implementation effort - _"How long does it take me to initially implement the test?"_
* maintenance effort - _"How often, and how much time, do I need to invest in maintaining the test?"_
* execution time - _"How much time is needed to execute the test?"_

By far the most important of these, for projects with long life spans, is _execution time_.
Test suites will be run thousands of times over the life span of a project.
Wasted minutes will accumulate to a lot of wasted time (= money) very fast.

A good test suite should be executable by developers anytime they need to make sure everything still works.
Without having to schedule a coffee break or meeting to kill time while waiting for the result.

=== On Labeling Tests

Humans tend to put things into mental boxes in order to easier make sense of them.
This also happens when we think about tests, as can be seen by all the different kinds of tests in the previous chapter.
There is just one problem:

_Things also tend to be more complicated and multi-faceted to just neatly fit into a box._

As an example, despite all attempts to standardise terms like _Integration-Test_ whenever you start to work with new people, you'll soon learn that they have a more or less different understanding of those terms than you.
Like spaces over tabs and other programming culture wars, this might never be resolved.

The good news is that labels are effectively worthless.
It does not matter what you call your tests.
Whatever they do, they should do it efficiently and mitigate some relevant risk.
The only reason to categorize your tests should be, to better communicate with your team when talking about them.
But this should be done with the conscious knowledge that whatever labels you define, might only be valid in your team's context.

Instead of wasting time arguing about how to name certain kinds of similar tests, invest that time into building common understanding about how your team wants to approach testing certain similar components.
(e.g. how to test repositories, controllers, cached method calls, etc.)

=== Examples of Tests

==== I want to make sure that the result of my pure function, given specific inputs, is correct.

* **scope:**
_function_

* **aspect under test:**
_functionality_

* **implementation effort:**
_Low_.
There are no dependencies to handle.
Permutation over possible input combinations is easy with most modern test automation frameworks.

* **maintenance effort:**
_Very low_.
Pure functions usually don't change a lot over time.
They are also self-contained and can therefore be tested as blackboxes.

* **execution time:**
_extremely fast_

The common label for this type of test is _Unit-Test_.
Which in this case should be rather uncontroversial.

==== I want to make sure that my repository class' SQL statements are syntactically correct and do what I expect them to.

* **scope:**
_class_

* **aspects under test:**
_functionality_ ("does what I want it to") & _integration_ ("SQL statements are syntactically correct")

* **implementation effort:**
Depending on the applied level of abstraction and the complexity of the underlying database, it varies between _low_ and _medium_.
Most of which usually comes from managing test data and therefore the complexity of the underlying database schema.

* **maintenance effort:**
Usually _low_, but bad abstractions can lead to unnecessary overhead - keep it simple!

* **execution time:**
Individual tests will be _very fast_.
The cost of the initial test setup depends on whether an in-memory database is a suitable replacement for the real thing.
If it is not, bootstrapping will take a couple of extra seconds (e.g. using Docker containers).

In regard to labeling your tests, this is a rather interesting example.
Some might label this type of test either as a _Unit-Test_ or an _Integration-Test_.

There are arguments for both sides.
If you think of _Integration-Test_ on the System-Component or Deployment-Unit level, then this is a _Unit-Test_.
If you think of _Integration-Test_ as anything that tests the integration of your code with anything else, then this is an _Integration-Test_.

In order to make things more clear, a more precise label would be _Technology Integration-Test_.
This specifies that its goal is to test that we are using a technology, in this case a database, correctly.

But as mentioned above, don't waste too much time labeling tests in the first place.

==== I want to make sure that the component responsible for talking to an HTTP API of another service runtime handles all relevant scenarios as intended.

* **scope:**
_class_ or _package_ (might include some function calls and helper classes as well as the main component)

* **aspects under test:**
_functionality_ ("handles all relevant scenarios") & _integration_ ("talking to an HTTP API")

* **implementation effort:**
Depending on the complexity of the API, it varies between _low_ and _medium_.
Most of which usually comes from managing the expected responses for all relevant scenarios.

* **maintenance effort:**
_Low_.
If the API is stable.
Otherwise, you'll have to change one thing or another every time the API changes.

* **execution time:**
Individual tests will be _very fast_.
Bootstrapping a service simulator will add about _another 1 to 2 seconds_ to the overall cost.

The goal is to test that the code is sending valid HTTP requests with the expected content, as well as that expected responses are handled correctly.
Writing a _Unit-Test_ and using mocks (e.g. for the HTTP client) will not actually test anything other than that the code is invoked as you've written it.
The most important aspect under test here is that the actually produced HTTP requests look as expected and that different responses are parsed and handled correctly.

None of which is possible without using an external simulator.

==== I want to make sure that an orchestrating service class behaves like it should even when exceptions occur.

* **scope:**
_class_

* **aspects under test:**
_functionality_ ("behaves like it should")

* **implementation effort:**
Depending on the complexity of the process being orchestrated (e.g. number of other components involved), it will vary between _low_ and _medium_.

* **maintenance effort:**
_Usually low_ when tests are implemented efficiently.
Higher if tests were written too close to the production code (white-box tests).

* **execution time:**
_extremely fast_

The goal is to verify behavioral aspects of the class under test.
This is done by initializing an instance of the class with most, if not all, dependencies mocked.
Dependencies include references to local resources (e.g. system clock) and other classes (e.g. event handler, repositories etc.).
Pure functions and other static calls should _not be mocked_!

Tests usually involve checking that the correct parts of the input data are given to the dependencies and that their results are in turn used correctly in the following steps.
If there are side effects (e.g. publishing of events) in the component's code, their invocation is also tested.

==== I want to make sure that security rules, like the way a user is authenticated, for certain paths of my HTTP-based API are enforced.

* **scope:**
_deployment unit_

* **aspects under test:**
_functionality_ ("works as intended"), _rules_ ("security rules") & _integration_ ("user is authenticated", "HTTP-based API")

* **implementation effort:**
If security is implemented in a test-friendly way, it _can be very low_.
If not it _might be much higher_.

* **maintenance effort:**
_Low_.
Once established, security rules do not change very often.

* **execution time:**
Individual tests will be _very fast_.
Bootstrapping the _deployment unit_ to start with the minimum set of components, to make the test meaningful might take a couple of seconds.

The goal is to test that certain security rules are applied for parts of an API using a certain authentication technology.
The security framework, the authentication protocols and HTTP as a transport layer are all technologies being integrated with your own code.

==== I want to make sure two of my service runtimes can talk to each other over HTTP and messages.

* **scope:**
_deployment units_

* **aspects under test:**
_functionality_ ("can talk") & _integration_ ("two of my service runtimes", "HTTP and messages")

* **implementation effort:**
_High_ if tests are implemented in a way that needs both service runtimes to be involved at the same time.
_Much lower_ if something like contracts (e.g. PACT, Spring Cloud Contract etc.) are used to decouple both services from each other.

* **maintenance effort:**
Every time something is changed by either of the deployment units, these tests need to be changed as well.
Depending on how the tests are set up (full integration vs. contract-based), the actual effort for each change might be _very low_ or _very high_.

* **execution time:**
Depends heavily on how the tests are set up.
Bootstrapping two deployment units and having to also set up test data for the target _can take a lot of time_.
Running against a simulation (e.g. contract-based integration) on the other hand is _very fast_.

This is an excellent example how knowing the scope and different aspects of what you want to test, and choosing the right tools to do so efficiently, makes the difference between tests running for minutes or just a few seconds.

Practices like having contracts for testing the integration of separate deployment units might just mitigate 90% of the risk.
But they are much more efficient than full integration tests which might mitigate 92% of the risk

==== I want to make sure, that certain architectural principals are followed in our codebase.

* **scope:**
_modules_ / _deployment unit_ / _repository_

* **aspects under test:**
following of (architectural) rules

* **implementation effort:**
_Medium_.
Tools like ArchUnit for the JVM make defining architectural rules and checking them as part of the regular tests easy.
Specifying more complex rules might take a while though.

* **maintenance effort:**
_Low_.
Architecture, once established, does not tend to change a lot over time.

* **execution time:**
Depends on the size of the code base.
Usually just a _couple of seconds_.
With the initial startup & analysis taking up most of the time.

=== Writing Automated Tests (Kotlin/Java)

Automated tests for Kotlin or Java usually involve a number of supporting libraries.
You'll need at least a testing framework - something that defines what a test is, how they are structured, how they are executed and what the overall lifecycle of executing tests actually involves.
Then you need something to assert if desired results or behaviour was actually achieved.
Testing for behaviour usually requires control over dependencies' which involves mocking components in order to define and record their behaviour.
Last but not least you'll need to simulate certain components that are not part of you code (e.g. databases, HTTP services, Message Brokers, etc).

==== Tooling

A very well-rounded technology stack for any Kotlin/Java project:

* testing framework: https://junit.org/junit5/[JUnit 5]
* assertions: https://assertj.github.io/doc/[AssertJ] - best for Java, also usable with Kotlin
* mocking: https://site.mockito.org[Mockito] for Java and https://mockk.io[MockK] for Kotlin
* simulators:
** https://wiremock.org[WireMock] for anything related HTTP communication
** https://www.h2database.com/html/main.html[H2] as an in memory replacement for SQL databases
** https://www.testcontainers.org[Testcontainers] for basically any technology that does not provide an in-memory variant

For alternative assertion libraries specifically for Kotlin, Novatec Consulting GmbH has an interesting https://www.novatec-gmbh.de/en/blog/kotlin-assertion-libraries-introduction/[blog series].

==== Test Structure

Automated JUnit tests are usually group by their scope into _test classes_ containing different _test methods_.
There are also ways of further grouping _test methods_ inside a _test class_.

_Test methods_ should be structured into 3 phases: _arrange_, _act_ & _assert_ (_AAA_)

In the _arrange_ phase test data is initialized, mocks are setup and state is initialized.
In the _act_ phase the _function, class, method or system under test_ is invoked.
In the _assert_ phase the result of _act_ incl. possible side-effects is asserted / verified.

We'll see more of this in the following examples.

==== Examples

Let's take a look at a very simple test class:

[source,kotlin]
----
import org.assertj.core.api.Assertions.assertThat
import org.junit.jupiter.api.Test

internal class CalculatorTests { // (1)

    val cut = Calculator() // (2)

    @Test // (3)
    fun `sum can handle any two positive numbers`() { // (4)
        assertThat(cut.sum(a = 1, b = 1)).isEqualTo(2)  // (5)
        assertThat(cut.sum(a = 9, b = 42)).isEqualTo(51)
        assertThat(cut.sum(a = 123456, b = 654321)).isEqualTo(777777)
    }
}

----

1. The scope of the tests inside the test class is the `Calculator` class.
The naming convention for test classes is _scope_ + `Test` or `Tests` suffix.
2. Usually a single instance of a _class under test_ (`cut`) is used for all tests inside the class.
However, if your _class under test_ has state oder needs to initialized in different variants, the `cut` property might also be a variable within each test method.
3. JUnit provides the `@Test` annotation for marking methods that should be treated as a single test.
Those methods mustn't have a result type (Kotlin: `Unit` or nothing; Java: `void`) and unless provided by a https://junit.org/junit5/docs/current/user-guide/#extensions-parameter-resolution[JUnit extension], no parameters are allowed.
4. Kotlin allows for naming test methods with spaces to build real sentences.
Java test methods need to be camel-cased.
Test names should be statements of (single) facts that, if the test fails, are proven wrong.
5. Using AssertJ we formulate assertions that need to be true, otherwise the test fails instantly.
Actually having a single test with multiple assertions basically asserting the same functionality is considered a bad practice.
If the first assertion fails, you'll not be able to tell if the other assertions had passed or failed.

The next example will show better alternatives of structuring this test:

[source,kotlin]
----
import org.assertj.core.api.Assertions.assertThat
import org.junit.jupiter.api.DynamicTest
import org.junit.jupiter.api.DynamicTest.dynamicTest
import org.junit.jupiter.api.TestFactory
import org.junit.jupiter.params.ParameterizedTest
import org.junit.jupiter.params.provider.CsvSource

internal class CalculatorTests {

    val cut = Calculator()

    @ParameterizedTest // (1)
    @CsvSource("1,1,2", "9,42,51", "123456,654321,777777") // (2)
    fun `sum can handle two positive numbers`(a: Int, b: Int, expected: Int) { // (3)
        assertThat(cut.sum(a = a, b = b)).isEqualTo(expected)
    }

    @TestFactory // (4)
    fun `sum can handle two positive numbers`(): List<DynamicTest> = // (5)
        listOf(
            Triple(1, 1, 2),
            Triple(9, 42, 51),
            Triple(123456, 654321, 777777)
        )
        .map { (a, b, expected) -> // (6)
            dynamicTest("$a + $b = $expected") {  // (7)
                assertThat(cut.sum(a = a, b = b)).isEqualTo(expected)
            }
        }
}
----

1. JUnit 5's https://junit.org/junit5/docs/current/user-guide/#writing-tests-parameterized-tests[parameterized test] annotation declares a special kind of test that actually support typed parameters to be injected into a test method.
These kinds of tests are ideal for permuting over different input values and expected results.
Each set of test data will be executed as its own test.
So each assertion can also fail on its own.
2. Parameterized tests need a source for the parameters.
There is quite a number of sources available to choose from.
In this case, because we want to provide 2 input values and an expectation, the CSV source allows us to define comma separated set of data.
3. The test parameters can be typed (in this case as `Int`) and are automatically converted from the sources `String` data.
4. A programmatic approach to this problem is provided by https://junit.org/junit5/docs/current/user-guide/#writing-tests-dynamic-tests[test factories].
These can be used to execute any code in order to produce `DynamicTest` instances, which are then invoked by JUnit like normal tests.
5. Methods annotated with `@TestFactory` are not allowed to have parameters (unless provided by a https://junit.org/junit5/docs/current/user-guide/#extensions-parameter-resolution[JUnit extension]) and need a return type of any `Collection<DynamicTest>` or `Stream<DynamicTest>`.
6. Kotlin allows for https://kotlinlang.org/docs/destructuring-declarations.html[destructuring] the test data to make it more readable.
7. The name for each dynamic test can be defined howerver you want.

For simple cases _parameterized tests_ are the preferred way.
_Test factories_ are a very powerful concept but are also genrally harder to read / understand.
They should only be used if none of the _parameter sources_ is enough to do whatever it is you need to do in your tests!

Finally, let's take a look at a much more complex example:

[source,kotlin]
----
import info.novatec.testit.logrecorder.api.LogRecord
import info.novatec.testit.logrecorder.assertion.LogRecordAssertion.Companion.assertThat
import info.novatec.testit.logrecorder.assertion.containsExactly
import info.novatec.testit.logrecorder.logback.junit5.RecordLoggers
import io.mockk.called
import io.mockk.clearAllMocks
import io.mockk.every
import io.mockk.mockk
import io.mockk.verify
import org.assertj.core.api.Assertions.assertThat
import org.junit.jupiter.api.BeforeEach
import org.junit.jupiter.api.DisplayName
import org.junit.jupiter.api.Nested
import org.junit.jupiter.api.Test
import org.springframework.util.IdGenerator
import java.time.Instant
import java.util.UUID.randomUUID

// test data
import starter.Examples.book_cleanCode
import starter.Examples.id_cleanArchitecture
import starter.Examples.id_cleanCode
import starter.Examples.record_cleanCode

internal class BookCollectionTest {

    val idGenerator: IdGenerator = mockk() // (1)
    val repository: BookRepository = mockk()
    val eventPublisher: BookEventPublisher = mockk(relaxUnitFun = true) // (2)

    val cut = BookCollection(idGenerator, repository, eventPublisher)

    @BeforeEach
    fun resetMocks() { // (3)
        clearMocks(idGenerator, repository, eventPublisher)
    }

    @Nested // (4)
    inner class GetById {

        @Test
        fun `returns the record if it was found`() { // (5)
            every { repository.findById(id_cleanCode) } returns record_cleanCode
            val bookRecord = cut.get(id_cleanCode)
            assertThat(bookRecord).isEqualTo(record_cleanCode)
        }

        @Test
        fun `returns null if it was not found`() {
            every { repository.findById(id_cleanCode) } returns null
            val bookRecord = cut.get(id_cleanCode)
            assertThat(bookRecord).isNull()
        }

    }

    @Nested
    inner class AddBook {

        val generatedId = randomUUID()
        val currentTimestamp = Instant.now()

        @BeforeEach
        fun stubDefaultBehaviour() {
            every { idGenerator.generateId() } returns generatedId // (6)
            every { repository.save(any()) } answers { simulateRepositorySave(firstArg()) } // (7)
        }

        @Test
        fun `returns a book record`() { // (8)
            val actualBookRecord = cut.add(book_cleanCode)
            val expectedBookRecord = BookRecord(generatedId, book_cleanCode, currentTimestamp)
            assertThat(actualBookRecord).isEqualTo(expectedBookRecord)
        }

        @Test
        fun `persists a book record it in the repository`() { // (9)
            cut.add(book_cleanCode)
            val unsavedBookRecord = BookRecord(generatedId, book_cleanCode)
            verify { repository.save(unsavedBookRecord) }
        }

        @Test
        fun `publishes a creation event`() { // (10)
            cut.add(book_cleanCode)

            val savedBookRecord = BookRecord(generatedId, book_cleanCode, currentTimestamp)
            val expectedEvent = BookRecordCreatedEvent(savedBookRecord)
            verify { eventPublisher.publish(expectedEvent) }
        }

        fun simulateRepositorySave(bookRecord: BookRecord): BookRecord =
            bookRecord.copy(timestamp = currentTimestamp)

    }

    @Nested
    inner class DeleteById {

        @Test
        fun `publishes a deletion event, if the record was actually deleted`() { // (11)
            every { repository.deleteById(id_cleanCode) } returns true
            cut.delete(id_cleanCode)
            verify { eventPublisher.publish(BookRecordDeletedEvent(id_cleanCode)) }
        }

        @Test
        fun `does not publish any event, if the record did was not actually deleted`() {
            every { repository.deleteById(id_cleanCode) } returns false
            cut.delete(id_cleanCode)
            verify { eventPublisher wasNot called }
        }

        @Test
        @RecordLoggers(BookCollection::class)
        fun `logs whether a book was actually deleted`(log: LogRecord) { // (12)
            every { repository.deleteById(id_cleanCode) } returns true
            every { repository.deleteById(id_cleanArchitecture) } returns false

            cut.delete(id_cleanCode)
            cut.delete(id_cleanArchitecture)

            assertThat(log) {
                containsExactly {
                    info("trying to delete book with ID '$id_cleanCode'")
                    debug("book with ID '$id_cleanCode' was deleted")
                    info("trying to delete book with ID '$id_cleanArchitecture'")
                    debug("book with ID '$id_cleanArchitecture' was not deleted")
                }
            }
        }
    }
}
----

1. We are using MockK to generate mocks for all dependencies of the _class under test_.
2. MockK allows defining certain behaviour when declaring a mock.
In this case we are instructing it to simply accept and record any interactions with methods that do not have a result type.
3. JUnit offers a variety of https://junit.org/junit5/docs/current/user-guide/#writing-tests-classes-and-methods[lifecycle methods] that allow, among other things, to execute code before and after each test.
In this case we reset the state of all mocks before each test.
4. As mention in the introduction, it is possible to group tests within a single test class.
The `@Nested` annotation can be used to define a _nested test class_.
All lifecycle methods of the surrounding class also apply for the nested class.
But lifecycle methods inside the nested class will not be used outside it.
5. Here you can see _AAA_ in action.
The `repository's` behaviour is defined (arranged).
Then the relevant method of the class under test is invoked (act).
Finally, the result of the invocation is asserted to match expectations.
6. Common default behaviour can be easily defined in a `@BeforeEach` method.
In this case the ID generator should always return the same ID.
7. MockK also allows for defining code (an "Answer") as behaviour.
This is very useful for more complex operations.
8. Different aspects of the same method should be tested with different test results.
Same as with not asserting permutations in the same test method.
Asserting multiple differnt aspects of a method in the same test would hide additional failures after the first assertion error.
The first test focuses on the result of the method invocation.
9. The second test checks behaviour.
In this case that the book record was actually persisted in the repository.
10. The third test makes sure that the correct application event was published.
11. The first two tests of this group show that even methods without a result type can still be tested for their behaviour.
12. This last test demonstrates how a custom extension can be used to inject something into a test.
In this case Novatec Consulting GmbH's own https://github.com/nt-ca-aqe/logrecorder[LogRecorder] used for checking if certain log entries were actually written.
